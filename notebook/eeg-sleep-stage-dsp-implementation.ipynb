{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9280571,"sourceType":"datasetVersion","datasetId":5617203}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Offline Setup & Locate Dataset (Recursive)","metadata":{}},{"cell_type":"code","source":"import os, numpy as np, matplotlib.pyplot as plt\nimport mne\nfrom scipy.signal import butter, filtfilt, iirnotch, welch\nfrom scipy.stats import skew, kurtosis\n\nINPUT_ROOT = \"/kaggle/input\"\nprint(\"Mounted inputs:\", [d for d in os.listdir(INPUT_ROOT) if os.path.isdir(os.path.join(INPUT_ROOT, d))])\n\nROOT_CAND = os.path.join(INPUT_ROOT, \"physiobank-database-sleep-edfx-cassette\")\nassert os.path.isdir(ROOT_CAND), \"Không thấy thư mục 'physiobank-database-sleep-edfx-cassette' sau khi Add Input.\"\n\n# Tìm tất cả thư mục con có .edf\nedf_dirs = {}\nfor root, dirs, files in os.walk(ROOT_CAND):\n    edfs = [f for f in files if f.lower().endswith(\".edf\")]\n    if edfs:\n        edf_dirs[root] = edfs\n\nif not edf_dirs:\n    raise FileNotFoundError(\"Không tìm thấy file .edf trong dataset đã gắn. Kiểm tra lại dataset đã Add.\")\n\n# Chọn thư mục có nhiều .edf nhất làm DATA_PATH\nDATA_PATH = max(edf_dirs.keys(), key=lambda k: len(edf_dirs[k]))\nfiles = sorted(edf_dirs[DATA_PATH])\n\nprint(\"DATA_PATH =\", DATA_PATH)\nprint(\"Total EDF files in this folder:\", len(files))\nprint(\"Sample files:\", files[:12])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:24.873655Z","iopub.execute_input":"2025-11-15T06:10:24.873940Z","iopub.status.idle":"2025-11-15T06:10:28.550504Z","shell.execute_reply.started":"2025-11-15T06:10:24.873911Z","shell.execute_reply":"2025-11-15T06:10:28.549651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Select PSG/Hypnogram Subset for Processing","metadata":{}},{"cell_type":"code","source":"SUBSET_PAIRS = [\n    (\"SC4001E0-PSG.edf\", \"SC4001EC-Hypnogram.edf\"),\n    (\"SC4002E0-PSG.edf\", \"SC4002EC-Hypnogram.edf\"),\n    (\"SC4011E0-PSG.edf\", \"SC4011EH-Hypnogram.edf\"),\n]\nfor psg, hyp in SUBSET_PAIRS:\n    assert psg in files and hyp in files, f\"Thiếu cặp: {psg} / {hyp}\"\nprint(\"Subset OK:\", SUBSET_PAIRS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:28.553052Z","iopub.execute_input":"2025-11-15T06:10:28.553443Z","iopub.status.idle":"2025-11-15T06:10:28.559002Z","shell.execute_reply.started":"2025-11-15T06:10:28.553419Z","shell.execute_reply":"2025-11-15T06:10:28.558056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load One PSG/Hyp Pair and Inspect Channels & Labels","metadata":{}},{"cell_type":"code","source":"psg_file = os.path.join(DATA_PATH, SUBSET_PAIRS[0][0])\nhyp_file = os.path.join(DATA_PATH, SUBSET_PAIRS[0][1])\n\nraw = mne.io.read_raw_edf(psg_file, preload=True, verbose=False)\nann = mne.read_annotations(hyp_file)\nprint(\"Channels:\", raw.ch_names)\nprint(\"Sampling rate:\", raw.info['sfreq'])\nprint(\"Unique annotations:\", sorted({a['description'] for a in ann})[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:28.559865Z","iopub.execute_input":"2025-11-15T06:10:28.560122Z","iopub.status.idle":"2025-11-15T06:10:35.151415Z","shell.execute_reply.started":"2025-11-15T06:10:28.560100Z","shell.execute_reply":"2025-11-15T06:10:35.150429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Select Main EEG Channel and Visualize Raw Signal","metadata":{}},{"cell_type":"code","source":"raw.pick(['EEG Fpz-Cz'])  \nfs = raw.info['sfreq']\n\nx_10s = raw.get_data()[0][:int(fs*10)]\nplt.figure(figsize=(10,3)); plt.plot(x_10s)\nplt.title(\"Raw EEG (Fpz-Cz) – first 10s\"); plt.xlabel(\"Sample\"); plt.ylabel(\"a.u.\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:35.152331Z","iopub.execute_input":"2025-11-15T06:10:35.152597Z","iopub.status.idle":"2025-11-15T06:10:35.530715Z","shell.execute_reply.started":"2025-11-15T06:10:35.152577Z","shell.execute_reply":"2025-11-15T06:10:35.529806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Digital Filtering (Band-pass 0.5–40 Hz + Notch 50 Hz)","metadata":{}},{"cell_type":"code","source":"def bandpass_filter(x, fs, lo=0.5, hi=40, order=4):\n    from scipy.signal import butter, filtfilt\n    b,a = butter(order, [lo/(fs/2), hi/(fs/2)], btype='band'); return filtfilt(b, a, x)\n\ndef notch_filter(x, fs, f0=50, Q=30):\n    from scipy.signal import iirnotch, filtfilt\n    b,a = iirnotch(f0/(fs/2), Q); return filtfilt(b, a, x)\n\nx = raw.get_data()[0]\nx_filt = notch_filter(bandpass_filter(x, fs, 0.5, 40, 4), fs, f0=50, Q=30)\n\nplt.figure(figsize=(10,3)); plt.plot(x_filt[:int(fs*10)])\nplt.title(\"Filtered EEG (first 10s)\"); plt.xlabel(\"Sample\"); plt.ylabel(\"a.u.\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:35.531575Z","iopub.execute_input":"2025-11-15T06:10:35.531896Z","iopub.status.idle":"2025-11-15T06:10:36.167247Z","shell.execute_reply.started":"2025-11-15T06:10:35.531865Z","shell.execute_reply":"2025-11-15T06:10:36.166519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Frequency Spectrum Analysis (FFT of EEG Signal)","metadata":{}},{"cell_type":"code","source":"from scipy.fft import fft, fftfreq\n\nN = int(10*fs)\nx_seg = x_filt[:N]\nY = fft(x_seg*np.hanning(N))\nF = fftfreq(N, 1/fs)\n\nplt.figure(figsize=(7,3))\nplt.plot(F[:N//2], 2.0/N*np.abs(Y[:N//2]))\nplt.xlim(0, 60)\nplt.xlabel(\"Frequency (Hz)\"); plt.ylabel(\"Amplitude\")\nplt.title(\"EEG Fpz–Cz Spectrum (10 s segment)\")\nplt.grid(True); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:36.168118Z","iopub.execute_input":"2025-11-15T06:10:36.168417Z","iopub.status.idle":"2025-11-15T06:10:36.324399Z","shell.execute_reply.started":"2025-11-15T06:10:36.168391Z","shell.execute_reply":"2025-11-15T06:10:36.323604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Discrete Filter Analysis: Butterworth Band-pass (0.5–40 Hz)","metadata":{}},{"cell_type":"code","source":"from scipy.signal import butter, freqz\n\nb, a = butter(4, [0.5/(fs/2), 40/(fs/2)], btype='band')\nw, h = freqz(b, a, worN=2048)\n\nplt.figure(figsize=(6,3))\nplt.plot((w/np.pi)*(fs/2), 20*np.log10(np.maximum(np.abs(h),1e-12)))\nplt.title(\"Butterworth Band-pass 0.5–40 Hz (order 4)\")\nplt.xlabel(\"Frequency (Hz)\"); plt.ylabel(\"Magnitude (dB)\"); plt.grid(True); plt.show()\n\np = np.roots(a); z = np.roots(b)\ntheta = np.linspace(0, 2*np.pi, 512)\nplt.figure(figsize=(3.5,3.5))\nplt.plot(np.cos(theta), np.sin(theta), 'k--')\nplt.scatter(z.real, z.imag, marker='o', label='Zeros')\nplt.scatter(p.real, p.imag, marker='x', label='Poles')\nplt.axis('equal'); plt.title(\"Pole–Zero Diagram\"); plt.legend(); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:36.326713Z","iopub.execute_input":"2025-11-15T06:10:36.326945Z","iopub.status.idle":"2025-11-15T06:10:36.695207Z","shell.execute_reply.started":"2025-11-15T06:10:36.326926Z","shell.execute_reply":"2025-11-15T06:10:36.694373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sampling Rate Reduction (Decimation & Anti-Aliasing Check)","metadata":{}},{"cell_type":"code","source":"from scipy.signal import firwin, lfilter\ntarget_fs = 80\nlp_cut = 35.0\nnumtaps = 255\nlp = firwin(numtaps, lp_cut/(fs/2))\nx_aa = lfilter(lp, [1.0], x_filt)\nfactor = int(round(fs/target_fs))\nx_dec = x_aa[::factor]\nfs_dec = fs/factor\nprint(\"Decimated sampling rate:\", fs_dec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:36.696138Z","iopub.execute_input":"2025-11-15T06:10:36.696927Z","iopub.status.idle":"2025-11-15T06:10:37.299513Z","shell.execute_reply.started":"2025-11-15T06:10:36.696902Z","shell.execute_reply":"2025-11-15T06:10:37.298788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EOG Artifact Removal using ICA (Multi-channel)","metadata":{}},{"cell_type":"code","source":"import mne\nfrom mne.preprocessing import ICA, create_eog_epochs\n\n# 1) đọc lại PSG dạng multi-channel (đừng dùng 'raw' đã pick 1 kênh)\nraw_multi = mne.io.read_raw_edf(psg_file, preload=True, verbose=False)\nfs_mc = raw_multi.info['sfreq']\n\n# 2) chọn kênh EEG+EOG nếu có\npicks = mne.pick_types(raw_multi.info, eeg=True, eog=True, exclude=[])\nn_ch = len(picks)\nprint(f\"[ICA] channels available: {n_ch}\")\n\ndef _bp_notch(x, fs):\n    # dùng cùng tham số lọc như Cell 5\n    from scipy.signal import butter, filtfilt, iirnotch\n    b,a = butter(4, [0.5/(fs/2), 40/(fs/2)], btype='band')\n    x = filtfilt(b,a,x)\n    b,a = iirnotch(50/(fs/2), 30)\n    x = filtfilt(b,a,x)\n    return x\n\nif n_ch < 2:\n    print(\"[ICA] <2 channels → skip ICA, use x_filt.\")\n    x_sig = x_filt.copy()\nelse:\n    # 3) high-pass 1 Hz trước khi fit ICA (khuyến nghị MNE)\n    raw_hp = raw_multi.copy().filter(l_freq=1.0, h_freq=None, picks=picks, method=\"iir\", verbose=False)\n\n    # 4) số thành phần ICA không vượt quá số kênh\n    n_comp = min(15, n_ch)\n    ica = ICA(n_components=n_comp, random_state=42, method='fastica')\n    ica.fit(raw_hp, picks=picks, verbose=False)\n    print(f\"[ICA] fitted with n_components={n_comp}\")\n\n    # 5) tìm thành phần liên quan EOG\n    has_eog = len(mne.pick_types(raw_multi.info, eog=True)) > 0\n    if has_eog:\n        eog_chs = [ch for ch in raw_hp.ch_names if 'EOG' in ch.upper()]\n        eog_epochs = create_eog_epochs(raw_hp, ch_name=(eog_chs[0] if eog_chs else None),\n                                       reject_by_annotation=True, verbose=False)\n        eog_inds, _ = ica.find_bads_eog(eog_epochs, threshold=2.0)\n    else:\n        # fallback: tương quan với EEG chính\n        eeg_main = 'EEG Fpz-Cz' if 'EEG Fpz-Cz' in raw_hp.ch_names else raw_hp.ch_names[mne.pick_types(raw_hp.info, eeg=True, exclude=[])[0]]\n        eog_inds, _ = ica.find_bads_eog(raw_hp, ch_name=eeg_main, threshold=2.0)\n\n    ica.exclude = eog_inds\n    print(\"[ICA] excluded comps:\", eog_inds)\n\n    # 6) áp dụng ICA và rút kênh EEG chính\n    raw_clean_mc = ica.apply(raw_hp.copy(), verbose=False)\n    eeg_name = 'EEG Fpz-Cz' if 'EEG Fpz-Cz' in raw_clean_mc.ch_names else raw_clean_mc.ch_names[mne.pick_types(raw_clean_mc.info, eeg=True, exclude=[])[0]]\n    raw_clean_eeg = raw_clean_mc.copy().pick([eeg_name])\n    x_clean0 = raw_clean_eeg.get_data()[0]\n\n    # 7) band-pass + notch (để nhất quán với pipeline) → x_sig\n    x_sig = _bp_notch(x_clean0, fs_mc)\n\nprint(\"x_sig ready. Downstream steps (epoch/features) will use x_sig.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:10:37.300305Z","iopub.execute_input":"2025-11-15T06:10:37.300551Z","iopub.status.idle":"2025-11-15T06:11:04.974583Z","shell.execute_reply.started":"2025-11-15T06:10:37.300525Z","shell.execute_reply":"2025-11-15T06:11:04.973734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Epoching (30 s Windows) and Label Mapping","metadata":{}},{"cell_type":"code","source":"EPOCH_SEC = 30\nlabel_map = {\n    'Sleep stage W':0, 'Sleep stage 1':1, 'Sleep stage 2':2,\n    'Sleep stage 3':3, 'Sleep stage 4':3, 'Sleep stage R':4\n}\n\n# đặt x_sig vào raw tạm của kênh EEG chính\nraw_tmp = raw.copy()                \nraw_tmp._data[0] = x_sig           \nraw_tmp.set_annotations(ann)\n\nevents, _ = mne.events_from_annotations(raw_tmp, event_id=label_map, verbose=False)\nepochs = mne.Epochs(raw_tmp, events, tmin=0, tmax=EPOCH_SEC, baseline=None, preload=True, verbose=False)\n\ny_subject = epochs.events[:, 2]\nimport pandas as pd\nprint(epochs)\nprint(\"Label counts:\", pd.Series(y_subject).value_counts().sort_index().to_dict())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:11:04.975382Z","iopub.execute_input":"2025-11-15T06:11:04.975628Z","iopub.status.idle":"2025-11-15T06:11:07.214003Z","shell.execute_reply.started":"2025-11-15T06:11:04.975609Z","shell.execute_reply":"2025-11-15T06:11:07.213193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Time–Frequency Analysis (STFT & Multitaper PSD)","metadata":{}},{"cell_type":"code","source":"x_ep = epochs.get_data()[0,0,:]\nplt.figure(figsize=(6,3))\nPxx, freqs, bins, im = plt.specgram(x_ep, NFFT=256, Fs=fs, noverlap=128)\nplt.ylim(0,40)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Hz\")\nplt.title(\"Spectrogram - Epoch 0\")\nplt.colorbar(label='Power')\nplt.show()\n\nNFFT = 256\nnover = int(0.75*NFFT)\nplt.figure(figsize=(6,3))\nPxx, freqs, bins, im = plt.specgram(x_ep, NFFT=NFFT, Fs=fs, noverlap=nover)\nplt.ylim(0,40); plt.xlabel(\"Time (s)\"); plt.ylabel(\"Hz\")\nplt.title(\"STFT (NFFT = 256, Overlap = 75%)\")\nplt.colorbar(label=\"Power\"); plt.show()\n\nfrom mne.time_frequency import psd_array_multitaper\npsd_mt, f_mt = psd_array_multitaper(x_ep, sfreq=fs, fmin=0.5, fmax=40.0,\n                                    adaptive=True, normalization='full', verbose=False)\nplt.figure(figsize=(6,3))\nplt.semilogy(f_mt, psd_mt)\nplt.xlabel(\"Hz\"); plt.ylabel(\"PSD\"); plt.title(\"Multitaper PSD – Sample Epoch\")\nplt.grid(True); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:11:07.214877Z","iopub.execute_input":"2025-11-15T06:11:07.215330Z","iopub.status.idle":"2025-11-15T06:11:08.369300Z","shell.execute_reply.started":"2025-11-15T06:11:07.215307Z","shell.execute_reply":"2025-11-15T06:11:08.368369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Advanced Feature Extractor (Time + Spectral + Rich DSP)","metadata":{}},{"cell_type":"code","source":"from scipy.signal import welch\nfrom scipy.stats import skew, kurtosis\nimport numpy as np\n\nBANDS = {\"delta\":(0.5,4), \"theta\":(4,8), \"alpha\":(8,12), \"beta\":(12,30), \"gamma\":(30,40)}\n\ndef rich_spectral_features(x, fs):\n    f, Pxx = welch(x, fs=fs, nperseg=int(4*fs))\n    m = (f>=0.5) & (f<=40); f=f[m]; P=Pxx[m]; P = P + 1e-18\n    total = np.trapz(P, f)\n    centroid  = np.trapz(f*P, f)/total\n    bandwidth = np.sqrt(np.trapz(((f-centroid)**2)*P, f)/total)\n    csum = np.cumsum(P)\n    median_f = f[np.searchsorted(csum, 0.5*csum[-1])]\n    sef90    = f[np.searchsorted(csum, 0.9*csum[-1])]\n    # 1/f correction (log-log detrend)\n    logf, logP = np.log(f), np.log(P)\n    A = np.vstack([logf, np.ones_like(logf)]).T\n    slope, intercept = np.linalg.lstsq(A, logP, rcond=None)[0]\n    P_corr = np.exp(logP - (slope*logf + intercept))\n    hf = np.trapz(P_corr[(f>=12)&(f<=30)], f[(f>=12)&(f<=30)])\n    lf = np.trapz(P_corr[(f>=0.5)&(f<=4)], f[(f>=0.5)&(f<=4)])\n    hf_lf_ratio = hf/(lf+1e-12)\n    return {\"spectral_centroid\":centroid, \"spectral_bandwidth\":bandwidth,\n            \"median_freq\":median_f, \"sef90\":sef90, \"hf_lf_1overf\":hf_lf_ratio}\n\ndef extract_features_epoch_ADV(x, fs):\n    # z-score per epoch\n    x = (x - np.mean(x)) / (np.std(x) + 1e-8)\n    feats = {\n        \"mean\": float(np.mean(x)),\n        \"std\":  float(np.std(x)),\n        \"var\":  float(np.var(x)),\n        \"skew\": float(skew(x)),\n        \"kurt\": float(kurtosis(x, fisher=True)),\n    }\n    # Hjorth\n    dx, ddx = np.diff(x), np.diff(np.diff(x))\n    vx = np.var(x) + 1e-12; vdx = np.var(dx) + 1e-12; vddx = np.var(ddx) + 1e-12\n    feats.update({\n        \"hjorth_activity\": vx,\n        \"hjorth_mobility\": np.sqrt(vdx/vx),\n        \"hjorth_complexity\": (np.sqrt(vddx/vdx)) / (np.sqrt(vdx/vx) + 1e-12),\n    })\n    # Welch band powers\n    f, Pxx = welch(x, fs=fs, nperseg=int(4*fs))\n    mask = (f>=0.5)&(f<=40); total = np.trapz(Pxx[mask], f[mask]) + 1e-12\n    for name,(lo,hi) in BANDS.items():\n        idx = (f>=lo)&(f<=hi)\n        p = np.trapz(Pxx[idx], f[idx])\n        feats[f\"pow_{name}_abs\"] = p\n        feats[f\"pow_{name}_rel\"] = p/total\n    # spectral entropy (custom, normalized)\n    P = Pxx[mask]; Pn = P/np.sum(P); H = -np.sum(Pn*np.log(Pn+1e-12))/np.log(len(Pn))\n    feats[\"spec_entropy\"] = float(H)\n    # rich spectral\n    feats.update(rich_spectral_features(x, fs))\n    return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:11:08.370106Z","iopub.execute_input":"2025-11-15T06:11:08.370379Z","iopub.status.idle":"2025-11-15T06:11:08.384479Z","shell.execute_reply.started":"2025-11-15T06:11:08.370349Z","shell.execute_reply":"2025-11-15T06:11:08.383643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Extract Features for All Epochs (One Subject)","metadata":{}},{"cell_type":"code","source":"# === Cell 14: features for one subject using ADV extractor ===\nX_subj, y_subj = [], []\nfor i in range(len(epochs)):\n    x_ep = epochs.get_data()[i, 0, :]   # 1 channel\n    feats = extract_features_epoch_ADV(x_ep, fs)\n    if i == 0:\n        feature_names = list(feats.keys())\n    X_subj.append([feats[k] for k in feature_names])\n    y_subj.append(int(epochs.events[i, 2]))\n\nX_subj = np.asarray(X_subj, dtype=np.float32)\ny_subj = np.asarray(y_subj, dtype=np.int64)\n\nprint(\"X_subj:\", X_subj.shape, \"| y_subj:\", y_subj.shape, \"| n_features:\", len(feature_names))\nprint(\"All finite? \", np.isfinite(X_subj).all())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:11:08.385219Z","iopub.execute_input":"2025-11-15T06:11:08.385441Z","iopub.status.idle":"2025-11-15T06:11:08.997288Z","shell.execute_reply.started":"2025-11-15T06:11:08.385423Z","shell.execute_reply":"2025-11-15T06:11:08.996246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Batch Processing & Save (Base vs ICA Datasets)","metadata":{}},{"cell_type":"code","source":"import json, pandas as pd, numpy as np\nimport mne\n\ndef process_one_pair(psg_name, hyp_name, use_ica=False):\n    psg_path = os.path.join(DATA_PATH, psg_name)\n    hyp_path = os.path.join(DATA_PATH, hyp_name)\n\n    # ---- read multi-channel of THIS subject (do NOT reuse global 'raw') ----\n    raw_mc = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)\n    fs_local = raw_mc.info['sfreq']\n\n    # choose main EEG name for this file\n    if 'EEG Fpz-Cz' in raw_mc.ch_names:\n        eeg_name = 'EEG Fpz-Cz'\n    else:\n        eeg_idx = mne.pick_types(raw_mc.info, eeg=True, exclude=[])[0]\n        eeg_name = raw_mc.ch_names[eeg_idx]\n\n    # --- helper: our pipeline band-pass + notch (same as earlier) ---\n    from scipy.signal import butter, filtfilt, iirnotch\n    def bp_notch(x, fs):\n        b,a = butter(4, [0.5/(fs/2), 40/(fs/2)], btype='band'); x = filtfilt(b,a,x)\n        b,a = iirnotch(50/(fs/2), 30);                               x = filtfilt(b,a,x)\n        return x\n\n    # --- get EEG series (optionally with ICA) *from this subject* ---\n    def get_eeg_series(use_ica_flag):\n        if not use_ica_flag:\n            sig0 = raw_mc.copy().pick([eeg_name]).get_data()[0]\n            return bp_notch(sig0, fs_local)\n\n        picks = mne.pick_types(raw_mc.info, eeg=True, eog=True, exclude=[])\n        if len(picks) < 2:\n            # not enough channels for ICA, fall back to base\n            sig0 = raw_mc.copy().pick([eeg_name]).get_data()[0]\n            return bp_notch(sig0, fs_local)\n\n        # high-pass 1 Hz then ICA\n        raw_hp = raw_mc.copy().filter(l_freq=1.0, h_freq=None, picks=picks, method=\"iir\", verbose=False)\n        n_comp = min(15, len(picks))\n        ica = mne.preprocessing.ICA(n_components=n_comp, random_state=42, method='fastica')\n        ica.fit(raw_hp, picks=picks, verbose=False)\n\n        has_eog = len(mne.pick_types(raw_mc.info, eog=True)) > 0\n        if has_eog:\n            eog_chs = [ch for ch in raw_hp.ch_names if 'EOG' in ch.upper()]\n            eog_epochs = mne.preprocessing.create_eog_epochs(raw_hp, ch_name=(eog_chs[0] if eog_chs else None),\n                                                             reject_by_annotation=True, verbose=False)\n            eog_inds, _ = ica.find_bads_eog(eog_epochs, threshold=2.0)\n        else:\n            eog_inds, _ = ica.find_bads_eog(raw_hp, ch_name=eeg_name, threshold=2.0)\n        ica.exclude = eog_inds\n\n        raw_clean_mc = ica.apply(raw_hp.copy(), verbose=False)\n        sig0 = raw_clean_mc.copy().pick([eeg_name]).get_data()[0]\n        return bp_notch(sig0, fs_local)\n\n    x_series = get_eeg_series(use_ica)\n\n    # ---- build raw_tmp FROM raw_mc (same subject, same length) ----\n    raw_eeg = raw_mc.copy().pick([eeg_name])      # single-channel Raw\n    assert raw_eeg.get_data().shape[1] == x_series.shape[0], \\\n        f\"Length mismatch: raw={raw_eeg.get_data().shape[1]} vs x_series={x_series.shape[0]}\"\n\n    raw_tmp = raw_eeg.copy()\n    raw_tmp._data[0] = x_series  # now lengths match\n\n    # annotations of THIS subject\n    ann = mne.read_annotations(os.path.join(DATA_PATH, hyp_name))\n    raw_tmp.set_annotations(ann)\n\n    # epoch 30 s with 5-class mapping\n    label_map = {'Sleep stage W':0,'Sleep stage 1':1,'Sleep stage 2':2,'Sleep stage 3':3,'Sleep stage 4':3,'Sleep stage R':4}\n    events, _ = mne.events_from_annotations(raw_tmp, event_id=label_map, verbose=False)\n    epochs_local = mne.Epochs(raw_tmp, events, tmin=0, tmax=30, baseline=None, preload=True, verbose=False)\n\n    # feature extraction (ADV)\n    X_list, y_list = [], []\n    for i in range(len(epochs_local)):\n        x_ep = epochs_local.get_data()[i,0,:]\n        feats = extract_features_epoch_ADV(x_ep, fs_local)\n        if i == 0: fn = list(feats.keys())\n        X_list.append([feats[k] for k in fn])\n        y_list.append(int(epochs_local.events[i,2]))\n\n    return np.array(X_list, np.float32), np.array(y_list, np.int64), fn\n\ndef run_batch(use_ica=False, tag=\"base\"):\n    X_all, y_all, feature_names, meta = None, None, None, []\n    for psg, hyp in SUBSET_PAIRS:\n        sid = psg.split('-')[0]\n        Xs, ys, fn = process_one_pair(psg, hyp, use_ica=use_ica)\n        if feature_names is None: feature_names = fn\n        X_all = Xs if X_all is None else np.vstack([X_all, Xs])\n        y_all = ys if y_all is None else np.concatenate([y_all, ys])\n        meta += [(sid, i, int(ys[i])) for i in range(len(ys))]\n\n    np.savez(f\"/kaggle/working/features_{tag}.npz\", X=X_all, y=y_all)\n    with open(f\"/kaggle/working/feature_names_{tag}.json\",\"w\") as f: json.dump(feature_names, f)\n    pd.DataFrame(meta, columns=[\"subject_id\",\"epoch_idx\",\"stage\"]).to_csv(f\"/kaggle/working/meta_{tag}.csv\", index=False)\n    print(f\"Saved: features_{tag}.npz, feature_names_{tag}.json, meta_{tag}.csv\")\n    print(\"Shapes:\", X_all.shape, y_all.shape)\n    return X_all, y_all\n\n# --- Run both variants for ablation ---\nXb, yb = run_batch(use_ica=False, tag=\"base\")\nXi, yi = run_batch(use_ica=True,  tag=\"ica\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:11:08.998119Z","iopub.execute_input":"2025-11-15T06:11:08.998377Z","iopub.status.idle":"2025-11-15T06:13:02.054149Z","shell.execute_reply.started":"2025-11-15T06:11:08.998356Z","shell.execute_reply":"2025-11-15T06:13:02.053302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CONFIG & IMPORT","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport os\nimport time\nimport json\n\nfrom tqdm.auto import tqdm\n\nfrom sklearn.model_selection import (\n    train_test_split, StratifiedKFold, cross_validate\n)\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, cohen_kappa_score, classification_report,\n    confusion_matrix, make_scorer\n)\n\n# --- Experiment Configurations ---\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nUSE_GROUP_SPLIT = False       # Set to True if meta has a 'subject' column\nCV_FOLDS = 5\nSCORING_MAIN = \"f1_macro\"     # main metric to rank models\nCLASS_WEIGHT = None           # or \"balanced\" if you prefer F1/Kappa\n\nDATA_PATH = \"\"                # Folder containing features_base.npz, meta_base.csv, ...\n\n# Scoring dict for cross-validation\nkappa_scorer = make_scorer(cohen_kappa_score)\nSCORING_DICT = {\n    \"accuracy\": \"accuracy\",\n    \"f1_macro\": \"f1_macro\",\n    \"cohen_kappa\": kappa_scorer,\n}\n\n# Create required folders\nos.makedirs(\"models\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\n\nprint(\"Configuration loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:02.055014Z","iopub.execute_input":"2025-11-15T06:13:02.055251Z","iopub.status.idle":"2025-11-15T06:13:02.747673Z","shell.execute_reply.started":"2025-11-15T06:13:02.055232Z","shell.execute_reply":"2025-11-15T06:13:02.746818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### READ DATA","metadata":{}},{"cell_type":"code","source":"def load_data(name):\n    \"\"\"Load X, labels, and feature names for each dataset.\"\"\"\n    # Load features\n    with np.load(os.path.join(DATA_PATH, f\"features_{name}.npz\")) as f:\n        X = f[\"X\"]\n\n    # Load metadata\n    df_meta = pd.read_csv(os.path.join(DATA_PATH, f\"meta_{name}.csv\"))\n    y_raw = df_meta[\"stage\"].values\n\n    # Load feature names\n    with open(os.path.join(DATA_PATH, f\"feature_names_{name}.json\"), \"r\") as f:\n        feature_names = json.load(f)\n\n    print(f\"{name.upper()} loaded: X={X.shape}, y={y_raw.shape}\")\n    return X, y_raw, feature_names\n\n\nX_base, y_base_raw, feature_names_base = load_data(\"base\")\nX_ica,  y_ica_raw,  feature_names_ica  = load_data(\"ica\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:02.748574Z","iopub.execute_input":"2025-11-15T06:13:02.749097Z","iopub.status.idle":"2025-11-15T06:13:02.768307Z","shell.execute_reply.started":"2025-11-15T06:13:02.749067Z","shell.execute_reply":"2025-11-15T06:13:02.767422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LABEL ENCODING","metadata":{}},{"cell_type":"code","source":"# Fit on union of Base ∪ ICA to ensure consistent mapping\nlabel_union = np.unique(np.concatenate([y_base_raw, y_ica_raw]))\nle = LabelEncoder()\nle.fit(label_union)\n\ny_base = le.transform(y_base_raw)\ny_ica  = le.transform(y_ica_raw)\n\nprint(\"Label mapping (string → ID):\")\nprint(dict(zip(le.classes_, le.transform(le.classes_))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:02.769197Z","iopub.execute_input":"2025-11-15T06:13:02.769466Z","iopub.status.idle":"2025-11-15T06:13:02.777350Z","shell.execute_reply.started":"2025-11-15T06:13:02.769447Z","shell.execute_reply":"2025-11-15T06:13:02.776540Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### TRAIN/TEST SPLIT","metadata":{}},{"cell_type":"code","source":"indices = np.arange(len(X_base))\n\ntrain_idx, test_idx = train_test_split(\n    indices,\n    test_size=TEST_SIZE,\n    random_state=RANDOM_STATE,\n    stratify=y_base\n)\n\nX_train_base, X_test_base = X_base[train_idx], X_base[test_idx]\ny_train_base, y_test_base = y_base[train_idx], y_base[test_idx]\n\nX_train_ica, X_test_ica = X_ica[train_idx], X_ica[test_idx]\ny_train_ica, y_test_ica = y_ica[train_idx], y_ica[test_idx]\n\nprint(f\"Split completed: Train={len(train_idx)}, Test={len(test_idx)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:02.778237Z","iopub.execute_input":"2025-11-15T06:13:02.778509Z","iopub.status.idle":"2025-11-15T06:13:02.801425Z","shell.execute_reply.started":"2025-11-15T06:13:02.778490Z","shell.execute_reply":"2025-11-15T06:13:02.800600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DEFINE MODELS & PIPELINES","metadata":{}},{"cell_type":"code","source":"# Base models (no tuning yet)\nMODELS = {\n    \"Logistic Regression\": LogisticRegression(\n        random_state=RANDOM_STATE,\n        max_iter=2000,\n        class_weight=CLASS_WEIGHT,\n        solver=\"lbfgs\",\n        multi_class=\"auto\"\n    ),\n    \"Random Forest\": RandomForestClassifier(\n        random_state=RANDOM_STATE,\n        n_estimators=300,\n        class_weight=CLASS_WEIGHT\n    ),\n    \"SVM (RBF)\": SVC(\n        kernel=\"rbf\",\n        class_weight=CLASS_WEIGHT\n    )\n}\n\n# Pipelines for baseline comparison (CV on BASE)\npipelines = {\n    name: Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"clf\", model)\n    ])\n    for name, model in MODELS.items()\n}\n\n# Hyperparameter grid for Logistic Regression (main model)\nLR_PARAM_GRID = {\n    \"C\": [0.001, 0.01, 0.1, 1, 10],\n    \"solver\": [\"liblinear\", \"lbfgs\"]\n}\n\nprint(\"Models, pipelines, and LR hyperparameter grid are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:02.802444Z","iopub.execute_input":"2025-11-15T06:13:02.802749Z","iopub.status.idle":"2025-11-15T06:13:02.817950Z","shell.execute_reply.started":"2025-11-15T06:13:02.802721Z","shell.execute_reply":"2025-11-15T06:13:02.817004Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CROSS-VALIDATION (BASE, ALL MODELS)","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n\ncv_results = []\n\nprint(f\"Running {CV_FOLDS}-fold cross-validation on BASE features for all models...\")\n\nfor name, pipe in pipelines.items():\n    print(f\"\\n>>> Model: {name}\")\n    start_time_cv = time.time()\n\n    scores = cross_validate(\n        pipe,\n        X_train_base,\n        y_train_base,\n        cv=cv,\n        scoring=SCORING_DICT,\n        n_jobs=-1\n    )\n\n    time_taken = time.time() - start_time_cv\n\n    acc_mean = scores[\"test_accuracy\"].mean()\n    f1_mean = scores[\"test_f1_macro\"].mean()\n    kappa_mean = scores[\"test_cohen_kappa\"].mean()\n\n    cv_results.append({\n        \"Model\": name,\n        \"accuracy_mean\": acc_mean,\n        \"f1_macro_mean\": f1_mean,\n        \"kappa_mean\": kappa_mean,\n        \"time_sec\": time_taken\n    })\n\n    print(f\"Accuracy (mean): {acc_mean:.4f}\")\n    print(f\"F1-macro (mean): {f1_mean:.4f}\")\n    print(f\"Cohen's kappa:  {kappa_mean:.4f}\")\n    print(f\"CV time:         {time_taken:.2f} s\")\n\ndf_cv = pd.DataFrame(cv_results)\nprint(\"\\n=== CV SUMMARY (BASE FEATURES) ===\")\nprint(df_cv)\n\n# Optionally, find the best model by F1_macro (for information)\nbest_by_f1 = df_cv.loc[df_cv[\"f1_macro_mean\"].idxmax()]\nprint(\"\\nBest model on CV (by F1_macro):\")\nprint(best_by_f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:02.818801Z","iopub.execute_input":"2025-11-15T06:13:02.819016Z","iopub.status.idle":"2025-11-15T06:13:05.723314Z","shell.execute_reply.started":"2025-11-15T06:13:02.819000Z","shell.execute_reply":"2025-11-15T06:13:05.722395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LR GRID SEARCH (WITH PROGRESS BAR)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid\n\nparam_list = list(ParameterGrid(LR_PARAM_GRID))\nbest_score = -np.inf\nbest_params = None\n\nprint(f\"Tuning Logistic Regression ({len(param_list)} combinations)...\")\n\nstart_time_lr_tuning = time.time()    # <-- START TIMER\n\nfor params in tqdm(param_list, desc=\"LR Grid Search\", unit=\"combination\"):\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"clf\", LogisticRegression(\n            random_state=RANDOM_STATE,\n            max_iter=2000,\n            class_weight=CLASS_WEIGHT,\n            **params\n        ))\n    ])\n\n    fold_scores = []\n    for tr_idx, val_idx in cv.split(X_train_base, y_train_base):\n        X_tr, X_val = X_train_base[tr_idx], X_train_base[val_idx]\n        y_tr, y_val = y_train_base[tr_idx], y_train_base[val_idx]\n\n        pipe.fit(X_tr, y_tr)\n        y_pred = pipe.predict(X_val)\n\n        f1 = f1_score(y_val, y_pred, average=\"macro\", zero_division=0)\n        fold_scores.append(f1)\n\n    mean_f1 = np.mean(fold_scores)\n\n    if mean_f1 > best_score:\n        best_score = mean_f1\n        best_params = params\n\n# <-- END TIMER\ntuning_time = time.time() - start_time_lr_tuning\n\nprint(\"\\nBest LR params:\", best_params)\nprint(\"Best CV F1_macro:\", best_score)\nprint(f\"Tuning time: {tuning_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:05.724266Z","iopub.execute_input":"2025-11-15T06:13:05.724637Z","iopub.status.idle":"2025-11-15T06:13:06.734110Z","shell.execute_reply.started":"2025-11-15T06:13:05.724605Z","shell.execute_reply":"2025-11-15T06:13:06.731828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### FINAL TRAIN (ALL MODELS, BASE & ICA)","metadata":{}},{"cell_type":"code","source":"results_base = {}\nresults_ica = {}\n\n# 1) Build final LR models (with best_params from Cell 6)\nbest_lr_base = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", LogisticRegression(\n        random_state=RANDOM_STATE,\n        max_iter=2000,\n        class_weight=CLASS_WEIGHT,\n        **best_params\n    ))\n])\n\nbest_lr_ica = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", LogisticRegression(\n        random_state=RANDOM_STATE,\n        max_iter=2000,\n        class_weight=CLASS_WEIGHT,\n        **best_params\n    ))\n])\n\n# 2) Build RF & SVM pipelines (using same configs as in MODELS)\nrf_base = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", RandomForestClassifier(\n        random_state=RANDOM_STATE,\n        n_estimators=300,\n        class_weight=CLASS_WEIGHT\n    ))\n])\n\nrf_ica = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", RandomForestClassifier(\n        random_state=RANDOM_STATE,\n        n_estimators=300,\n        class_weight=CLASS_WEIGHT\n    ))\n])\n\nsvm_base = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", SVC(\n        kernel=\"rbf\",\n        class_weight=CLASS_WEIGHT\n    ))\n])\n\nsvm_ica = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", SVC(\n        kernel=\"rbf\",\n        class_weight=CLASS_WEIGHT\n    ))\n])\n\n# 3) Fit all models on BASE\nstart_train_base = time.time()\nbest_lr_base.fit(X_train_base, y_train_base)\nrf_base.fit(X_train_base, y_train_base)\nsvm_base.fit(X_train_base, y_train_base)\ntrain_time_base = time.time() - start_train_base\n\n# 4) Fit all models on ICA\nstart_train_ica = time.time()\nbest_lr_ica.fit(X_train_ica, y_train_ica)\nrf_ica.fit(X_train_ica, y_train_ica)\nsvm_ica.fit(X_train_ica, y_train_ica)\ntrain_time_ica = time.time() - start_train_ica\n\n# 5) Evaluate helper\ndef eval_model(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    return {\n        \"accuracy\": accuracy_score(y_test, y_pred),\n        \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n        \"f1_weighted\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n        \"kappa\": cohen_kappa_score(y_test, y_pred),\n        \"y_pred\": y_pred,\n    }\n\n# 6) Evaluate all on BASE\nresults_base[\"Logistic Regression\"] = eval_model(best_lr_base, X_test_base, y_test_base)\nresults_base[\"Random Forest\"]      = eval_model(rf_base,     X_test_base, y_test_base)\nresults_base[\"SVM (RBF)\"]          = eval_model(svm_base,    X_test_base, y_test_base)\n\n# 7) Evaluate all on ICA\nresults_ica[\"Logistic Regression\"] = eval_model(best_lr_ica, X_test_ica, y_test_ica)\nresults_ica[\"Random Forest\"]       = eval_model(rf_ica,      X_test_ica, y_test_ica)\nresults_ica[\"SVM (RBF)\"]           = eval_model(svm_ica,     X_test_ica, y_test_ica)\n\n# 8) Print summary\nprint(\"\\n=== FINAL TEST RESULTS (BASE) ===\")\nfor name, res in results_base.items():\n    print(f\"\\nModel: {name}\")\n    print(f\"Accuracy:      {res['accuracy']:.4f}\")\n    print(f\"F1_macro:      {res['f1_macro']:.4f}\")\n    print(f\"F1_weighted:   {res['f1_weighted']:.4f}\")\n    print(f\"Cohen's Kappa: {res['kappa']:.4f}\")\n\nprint(\"\\n=== FINAL TEST RESULTS (ICA) ===\")\nfor name, res in results_ica.items():\n    print(f\"\\nModel: {name}\")\n    print(f\"Accuracy:      {res['accuracy']:.4f}\")\n    print(f\"F1_macro:      {res['f1_macro']:.4f}\")\n    print(f\"F1_weighted:   {res['f1_weighted']:.4f}\")\n    print(f\"Cohen's Kappa: {res['kappa']:.4f}\")\n\n# 9) Determine best model by F1_macro\nbest_model_base_name = max(results_base.items(), key=lambda x: x[1][\"f1_macro\"])[0]\nbest_model_ica_name  = max(results_ica.items(),  key=lambda x: x[1][\"f1_macro\"])[0]\n\nprint(f\"\\nBest model on BASE (by F1_macro): {best_model_base_name}\")\nprint(f\"Best model on ICA  (by F1_macro): {best_model_ica_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:06.738269Z","iopub.execute_input":"2025-11-15T06:13:06.738551Z","iopub.status.idle":"2025-11-15T06:13:08.304501Z","shell.execute_reply.started":"2025-11-15T06:13:06.738530Z","shell.execute_reply":"2025-11-15T06:13:08.303571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VISUALIZATION","metadata":{}},{"cell_type":"code","source":"def plot_cm(y_true, y_pred, title, path):\n    cm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n    plt.figure(figsize=(7,6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\".2f\",\n                xticklabels=le.classes_, yticklabels=le.classes_)\n    plt.title(title)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    plt.savefig(path)\n    plt.show()\n\n# Bar chart: F1_macro Base vs ICA\nmodel_names = list(results_base.keys())\nf1_base = [results_base[m][\"f1_macro\"] for m in model_names]\nf1_ica  = [results_ica[m][\"f1_macro\"]  for m in model_names]\n\nx = np.arange(len(model_names))\nwidth = 0.35\n\nplt.figure(figsize=(8,6))\nplt.bar(x - width/2, f1_base, width, label=\"BASE\")\nplt.bar(x + width/2, f1_ica,  width, label=\"ICA\")\nplt.xticks(x, model_names, rotation=15)\nplt.ylabel(\"F1-macro\")\nplt.title(\"Model Comparison: BASE vs ICA (F1-macro)\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"results/f1macro_base_vs_ica.png\")\nplt.show()\n\n# Confusion matrix for best model (BASE)\nbest_base_pred = results_base[best_model_base_name][\"y_pred\"]\nplot_cm(\n    y_test_base,\n    best_base_pred,\n    f\"BASE | Best model: {best_model_base_name}\",\n    \"results/cm_base_best.png\"\n)\n\n# Confusion matrix for best model (ICA)\nbest_ica_pred = results_ica[best_model_ica_name][\"y_pred\"]\nplot_cm(\n    y_test_ica,\n    best_ica_pred,\n    f\"ICA | Best model: {best_model_ica_name}\",\n    \"results/cm_ica_best.png\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:08.305456Z","iopub.execute_input":"2025-11-15T06:13:08.305716Z","iopub.status.idle":"2025-11-15T06:13:09.419593Z","shell.execute_reply.started":"2025-11-15T06:13:08.305676Z","shell.execute_reply":"2025-11-15T06:13:09.418870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_log = {\n    \"lr_grid_search_time_sec\": tuning_time,             \n    \"lr_final_train_base_sec\": train_time_base,         \n    \"lr_final_train_ica_sec\": train_time_ica,           \n}\n\ntraining_log[\"total_pipeline_time_sec\"] = (\n    training_log[\"lr_grid_search_time_sec\"]\n    + training_log[\"lr_final_train_base_sec\"]\n    + training_log[\"lr_final_train_ica_sec\"]\n)\n\nfor k, v in training_log.items():\n    print(f\"{k:<35}: {v:.2f} s\")\n\n# save to json\njson.dump(training_log, open(\"results/training_time_log.json\", \"w\"), indent=4)\n\nprint(\"\\nTraining time log saved to results/training_time_log.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:09.420426Z","iopub.execute_input":"2025-11-15T06:13:09.420650Z","iopub.status.idle":"2025-11-15T06:13:09.427487Z","shell.execute_reply.started":"2025-11-15T06:13:09.420633Z","shell.execute_reply":"2025-11-15T06:13:09.426632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SAVE RESULTS","metadata":{}},{"cell_type":"code","source":"json.dump(best_params, open(\"results/best_params.json\", \"w\"), indent=4)\nprint(\"Saved best_params.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:09.428315Z","iopub.execute_input":"2025-11-15T06:13:09.428585Z","iopub.status.idle":"2025-11-15T06:13:09.448041Z","shell.execute_reply.started":"2025-11-15T06:13:09.428566Z","shell.execute_reply":"2025-11-15T06:13:09.446908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SAVE MODELS","metadata":{}},{"cell_type":"code","source":"def safe_name(name: str) -> str:\n    \"\"\"Convert model name into a filesystem-safe string.\"\"\"\n    return (\n        name.lower()\n        .replace(\" \", \"_\")\n        .replace(\"(\", \"\")\n        .replace(\")\", \"\")\n        .replace(\"/\", \"_\")\n    )\n\n# Map model names to actual trained model objects (BASE)\nmodels_base = {\n    \"Logistic Regression\": best_lr_base,\n    \"Random Forest\": rf_base,\n    \"SVM (RBF)\": svm_base,\n}\n\n# Map model names to actual trained model objects (ICA)\nmodels_ica = {\n    \"Logistic Regression\": best_lr_ica,\n    \"Random Forest\": rf_ica,\n    \"SVM (RBF)\": svm_ica,\n}\n\n# Save all models for BASE\njoblib.dump(best_lr_base, \"models/logistic_regression_base.pkl\")\njoblib.dump(rf_base,       \"models/random_forest_base.pkl\")\njoblib.dump(svm_base,      \"models/svm_rbf_base.pkl\")\n\n# Save all models for ICA\njoblib.dump(best_lr_ica, \"models/logistic_regression_ica.pkl\")\njoblib.dump(rf_ica,      \"models/random_forest_ica.pkl\")\njoblib.dump(svm_ica,     \"models/svm_rbf_ica.pkl\")\n\nprint(\"Saved all models for BASE and ICA.\")\n\n# Also mark and save the best models explicitly (by F1_macro)\nbest_base_filename = f\"models/best_model_base_{safe_name(best_model_base_name)}.pkl\"\nbest_ica_filename  = f\"models/best_model_ica_{safe_name(best_model_ica_name)}.pkl\"\n\njoblib.dump(models_base[best_model_base_name], best_base_filename)\njoblib.dump(models_ica[best_model_ica_name],   best_ica_filename)\n\nprint(f\"Best BASE model ({best_model_base_name}) saved to: {best_base_filename}\")\nprint(f\"Best ICA  model ({best_model_ica_name}) saved to: {best_ica_filename}\")\n\n# Optionally save a small JSON summary of which model is best\nbest_model_summary = {\n    \"base\": {\n        \"best_model_name\": best_model_base_name,\n        \"file\": best_base_filename,\n    },\n    \"ica\": {\n        \"best_model_name\": best_model_ica_name,\n        \"file\": best_ica_filename,\n    },\n}\n\nwith open(\"results/best_models_summary.json\", \"w\") as f:\n    json.dump(best_model_summary, f, indent=4)\n\nprint(\"Best model summary saved to results/best_models_summary.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:09.449054Z","iopub.execute_input":"2025-11-15T06:13:09.449408Z","iopub.status.idle":"2025-11-15T06:13:09.664658Z","shell.execute_reply.started":"2025-11-15T06:13:09.449373Z","shell.execute_reply":"2025-11-15T06:13:09.663849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VERIFY MODEL EXPORT","metadata":{}},{"cell_type":"code","source":"# Load best model summary\nwith open(\"results/best_models_summary.json\", \"r\") as f:\n    best_info = json.load(f)\n\nbest_base_file = best_info[\"base\"][\"file\"]\nbest_ica_file  = best_info[\"ica\"][\"file\"]\n\nprint(f\"Best BASE model file: {best_base_file}\")\nprint(f\"Best ICA  model file: {best_ica_file}\")\n\n# Load best BASE model\nbest_base_model = joblib.load(best_base_file)\nsample_base = X_test_base[0].reshape(1, -1)\npred_base_id = best_base_model.predict(sample_base)[0]\npred_base_label = le.inverse_transform([pred_base_id])[0]\n\nprint(\"\\n[BASE] Single-sample prediction:\")\nprint(f\"Predicted ID:    {pred_base_id}\")\nprint(f\"Predicted label: {pred_base_label}\")\n\n# Load best ICA model\nbest_ica_model = joblib.load(best_ica_file)\nsample_ica = X_test_ica[0].reshape(1, -1)\npred_ica_id = best_ica_model.predict(sample_ica)[0]\npred_ica_label = le.inverse_transform([pred_ica_id])[0]\n\nprint(\"\\n[ICA] Single-sample prediction:\")\nprint(f\"Predicted ID:    {pred_ica_id}\")\nprint(f\"Predicted label: {pred_ica_label}\")\n\nprint(\"\\nModel export verification completed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:09.665514Z","iopub.execute_input":"2025-11-15T06:13:09.665756Z","iopub.status.idle":"2025-11-15T06:13:09.678842Z","shell.execute_reply.started":"2025-11-15T06:13:09.665733Z","shell.execute_reply":"2025-11-15T06:13:09.677955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EXPORT LABEL MAP","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\nprint(\"=== EXPORT LABEL MAP ===\")\n\n# Original mapping (string label -> numeric ID)\nlabel_map = {\n    'Sleep stage W': 0,\n    'Sleep stage 1': 1,\n    'Sleep stage 2': 2,\n    'Sleep stage 3': 3,\n    'Sleep stage 4': 3,   # merged into N3 (AASM standard)\n    'Sleep stage R': 4\n}\n\n# Build inverse mapping (numeric ID -> label string)\ninverse_label_map = {}\nfor label_str, label_id in label_map.items():\n    # Keep only the first occurrence for each numeric ID\n    if label_id not in inverse_label_map:\n        inverse_label_map[label_id] = label_str\n\n# Convert integer keys to string keys for JSON compatibility\ninverse_label_map_str_keys = {str(k): v for k, v in inverse_label_map.items()}\n\nprint(\"--- Inverse Label Map (ID → Label) ---\")\nprint(inverse_label_map_str_keys)\n\n# Save to results/label_map.json\noutput_path = \"results/label_map.json\"\n\ntry:\n    with open(output_path, \"w\") as f:\n        json.dump(inverse_label_map_str_keys, f, indent=4)\n    print(f\"\\n Label mapping successfully saved at: {output_path}\")\nexcept FileNotFoundError:\n    print(\"\\n WARNING: 'results/' directory not found. Please create it before saving.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T06:13:09.679852Z","iopub.execute_input":"2025-11-15T06:13:09.680304Z","iopub.status.idle":"2025-11-15T06:13:09.700122Z","shell.execute_reply.started":"2025-11-15T06:13:09.680275Z","shell.execute_reply":"2025-11-15T06:13:09.699336Z"}},"outputs":[],"execution_count":null}]}